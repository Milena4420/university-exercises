{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base MDP class\n",
    "\n",
    "class MDP:\n",
    "    def __init__(self):\n",
    "        \n",
    "        # empty transition function\n",
    "        self.P = defaultdict(dict)\n",
    "        # empty reward function\n",
    "        self.R = {}\n",
    "        \n",
    "        \n",
    "        self._states = None\n",
    "        \n",
    "    \n",
    "    def states(self):\n",
    "        if self._states is None:\n",
    "            self._states = [\n",
    "                s\n",
    "            for s, a in self.P.keys()\n",
    "            ]\n",
    "        return self._states\n",
    "    \n",
    "    def actions(self):\n",
    "        return []\n",
    "\n",
    "    def transition_probabilities(self, state, action):\n",
    "        return self.P[state, action].items()\n",
    "\n",
    "    def rewards(self, state, action):\n",
    "        return self.R[state, action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dungeon MDP class (complete the transition probabilities)\n",
    "\n",
    "def pos_for_action(x, y, action):\n",
    "    if action == \"right\":\n",
    "        return x+1, y\n",
    "    if action == \"left\":\n",
    "        return x-1, y\n",
    "    if action == \"up\":\n",
    "        return x, y+1\n",
    "    if action == \"down\":\n",
    "        return x, y-1\n",
    "\n",
    "class DungeonMDP(MDP):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "                \n",
    "\n",
    "        self.walls = [\n",
    "            (0,2), (2,2), (2,3), (2, 0) \n",
    "        ]\n",
    "        for i in range(5):\n",
    "            self.walls.append((-1, i))\n",
    "            self.walls.append((5, i))\n",
    "            self.walls.append((i, -1))\n",
    "            self.walls.append((i, 5))\n",
    "        \n",
    "\n",
    "        self.holes = [\n",
    "            (0, 4), (3, 2)\n",
    "        ]\n",
    "        \n",
    "        self.stairs = (3, 4)\n",
    "                \n",
    "        x_w, y_w = 4, 1\n",
    "        \n",
    "        self.weird_thing_path = [\n",
    "            (4, 1), (3, 1), (2, 1), (1, 1), (1, 2), (1, 3), (1, 4), (2, 4), (3, 4),\n",
    "            (4, 4), (4, 3), (4, 2)\n",
    "        ]\n",
    "        path_length = len(self.weird_thing_path)\n",
    "        \n",
    "        # terminal states\n",
    "        for action in self.actions():\n",
    "            for state, reward in [(\"TERMINAL\", 0), (\"WEIRD_THING\", -100), (\"HOLE\", -100), (\"STAIRS\", 0)]:\n",
    "                # all terminal states are \"absorbing\" states (you can't escape from them)\n",
    "                self.P[state, action] = {\"TERMINAL\": 1}\n",
    "                self.R[state, action] = reward\n",
    "        \n",
    "        # TODO: fill the probabilities\n",
    "        \n",
    "        for x in range(5):\n",
    "            for y in range(5):\n",
    "                for path_idx in range(path_length):\n",
    "                    for action in self.actions():\n",
    "                        adj = pos_for_action(x, y, action)\n",
    "                        \n",
    "                        x_w, y_w = self.weird_thing_path[path_idx]\n",
    "                        curr_state = (x, y, x_w, y_w)\n",
    "                        \n",
    "                        # -1 reward for any step taken in the maze\n",
    "                        self.R[curr_state, action] = -1\n",
    "                        \n",
    "                        for w_prob, next_path_idx in self.weird_thing_actions(path_idx):\n",
    "                            next_x_w, next_y_w = self.weird_thing_path[next_path_idx]\n",
    "                            \n",
    "                            if adj in self.walls:\n",
    "                                next_x, next_y = x, y\n",
    "                            else:\n",
    "                                next_x, next_y = adj\n",
    "                            \n",
    "                            # This next state, by default, has w_prob chance to happen given our action\n",
    "                            next_state = (next_x, next_y, next_x_w, next_y_w)\n",
    "                            \n",
    "                            if (next_x, next_y) in self.holes:\n",
    "                                # we are moving into a hole tile\n",
    "                                # 50% to fall and 50% to continue.\n",
    "                                self.P[curr_state, action][\"HOLE\"] = ...\n",
    "                                \n",
    "                                self.P[curr_state, action][next_state] = ...\n",
    "\n",
    "                            elif (next_x, next_y) == self.stairs:\n",
    "                                # we are moving into stairs\n",
    "                                \n",
    "                                # if the weird thing is also at the stairs, then we lose\n",
    "                                if (next_x_w, next_y_w) == self.stairs:\n",
    "                                    self.P[curr_state, action][\"WEIRD_THING\"] = ...\n",
    "                                else:\n",
    "                                    self.P[curr_state, action][\"STAIRS\"] = ...\n",
    "                            \n",
    "                            elif (next_x, next_y) == (next_x_w, next_y_w):\n",
    "                                # we encountered the weird thing\n",
    "                                self.P[curr_state, action][\"WEIRD_THING\"] = ...\n",
    "                                \n",
    "                            else:\n",
    "                                # we have 100% chance to move to the next location otherwise\n",
    "                                # the probability of the next state is just given by the probability of\n",
    "                                # the weird thing moving\n",
    "                                self.P[curr_state, action][next_state] = ...\n",
    "                                \n",
    "    def actions(self):\n",
    "        return [\"right\", \"left\", \"up\", \"down\"]\n",
    "\n",
    "    def weird_thing_actions(self, idx):\n",
    "    # 5% to not move, 75% to move clockwise, 20 to move counter-clockwise\n",
    "        return [\n",
    "        (0.05, idx), \n",
    "        (0.75, (idx+1)%len(self.weird_thing_path)), \n",
    "        (0.20, idx-1)\n",
    "        ]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backwards Induction Algorithm (to complete)\n",
    "\n",
    "def backwards_induction(mdp: MDP, horizon: int) -> Tuple[List[dict], List[dict]]:\n",
    "    # This function should return the optimal value function and Q function over timesteps.\n",
    "    \n",
    "    Qs = []\n",
    "    Vs = []\n",
    "    \n",
    "    # Initialize V_T(s) = 0 for all s\n",
    "    Vtp1 = {s: 0.0 for s in mdp.states()} # V_{t+1}\n",
    "    Vs.append(Vtp1)\n",
    "\n",
    "    for t in reversed(range(horizon)):\n",
    "        Qt = {}\n",
    "        Vt = {}\n",
    "        \n",
    "        # TODO: complete\n",
    "        # We should compute the Qt and Vt, by looping over states, actions, and next states.\n",
    "        \n",
    "        Qs.insert(0, Qt)\n",
    "        Vs.insert(0, Vt)\n",
    "        Vtp1 = Vt\n",
    "    \n",
    "    return Qs, Vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal policy computation\n",
    "def compute_policy(mdp: MDP, Qs: List[dict]) -> List[dict]:\n",
    "    # This function returns the policy didacted by the given Q function.\n",
    "    \n",
    "    policy = []\n",
    "    \n",
    "    for Qt in Qs:\n",
    "        pi_t = {}\n",
    "        for state in mdp.states():\n",
    "            best_action = max(mdp.actions(), key=lambda a: Qt[state, a])\n",
    "            pi_t[state] = best_action\n",
    "        policy.append(pi_t)\n",
    "            \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation utils\n",
    "\n",
    "action_to_vector = {\n",
    "    \"up\": (0, 0.3),\n",
    "    \"down\": (0, -0.3),\n",
    "    \"left\": (-0.3, 0),\n",
    "    \"right\": (0.3, 0),\n",
    "}\n",
    "\n",
    "def v_pi_heatmap(Vs, policy, timestep: int, weird_thing_pos: tuple, dungeon: DungeonMDP):\n",
    "    \"\"\"\n",
    "    Plots a heatmap for the value function V at a given timestep.\n",
    "    \n",
    "    \"\"\"\n",
    "    heatmap = np.full((5, 5), np.nan)\n",
    "    \n",
    "    Vt = Vs[timestep]\n",
    "    pi_t = policy[timestep]\n",
    "    \n",
    "    x_w, y_w = weird_thing_pos\n",
    "\n",
    "    for x in range(5):\n",
    "        for y in range(5):\n",
    "            key = (x, y, x_w, y_w)\n",
    "            if key in Vt:\n",
    "                heatmap[y, x] = Vt[key]  # Note: y first due to matplotlib row/col format\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    cmap = plt.cm.viridis\n",
    "    im = ax.imshow(heatmap, cmap=cmap, origin='lower')\n",
    "\n",
    "    ax.set_xticks(np.arange(5))\n",
    "    ax.set_yticks(np.arange(5))\n",
    "    ax.set_xticklabels(range(5))\n",
    "    ax.set_yticklabels(range(5))\n",
    "    \n",
    "    for (x, y) in dungeon.walls:\n",
    "        ax.add_patch(plt.Rectangle((x - 0.5, y - 0.5), 1, 1, color='black'))\n",
    "\n",
    "    for (x, y) in dungeon.holes:\n",
    "        ax.add_patch(plt.Rectangle((x - 0.5, y - 0.5), 1, 1, facecolor='none', edgecolor='red', hatch='O', linewidth=1))\n",
    "\n",
    "    x, y = dungeon.stairs\n",
    "    ax.add_patch(plt.Rectangle((x - 0.5, y - 0.5), 1, 1, facecolor='none', edgecolor='green', hatch='*', linewidth=1.5))\n",
    "\n",
    "    ax.plot(x_w, y_w, marker='x', markersize=10, color='red', label=\"Weird Thing\")\n",
    "    \n",
    "    for x in range(5):\n",
    "        for y in range(5):\n",
    "\n",
    "            key = x, y, x_w, y_w\n",
    "            if key in pi_t:\n",
    "                action = pi_t[key]\n",
    "                dx, dy = action_to_vector[action]\n",
    "                ax.arrow(x, y, dx, dy, head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "\n",
    "    ax.set_title(f\"Value Function and policy at t={timestep}\")\n",
    "    ax.legend(loc='upper right')\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = DungeonMDP()\n",
    "T = 25\n",
    "Qs, Vs = backwards_induction(mdp, T)\n",
    "policy = compute_policy(mdp, Qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to test and visualise the learned policy/Value function\n",
    "def simulate(Vs, policy, mdp):\n",
    "        \n",
    "    idx = 0\n",
    "    w_pos = mdp.weird_thing_path[idx]\n",
    "    for t in range(1, T+1):\n",
    "        \n",
    "        v_pi_heatmap(Vs, policy, t, w_pos, mdp)\n",
    "        # simulate weird thing moving\n",
    "        w_action_dist = mdp.weird_thing_actions(idx)\n",
    "        idx = np.random.choice([d[1] for d in w_action_dist], p=[d[0] for d in w_action_dist])\n",
    "        w_pos = mdp.weird_thing_path[idx]\n",
    "\n",
    "simulate(Vs, policy, mdp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
