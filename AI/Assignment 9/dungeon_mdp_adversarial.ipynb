{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base MDP class\n",
    "\n",
    "class MDP:\n",
    "    def __init__(self):\n",
    "        \n",
    "        # empty transition function\n",
    "        self.P = defaultdict(dict)\n",
    "        # empty reward function\n",
    "        self.R = {}\n",
    "        \n",
    "        \n",
    "        self._states = None\n",
    "        \n",
    "    \n",
    "    def states(self):\n",
    "        if self._states is None:\n",
    "            self._states = [\n",
    "                s\n",
    "            for s, a in self.P.keys()\n",
    "            ]\n",
    "        return self._states\n",
    "    \n",
    "    def actions(self):\n",
    "        return []\n",
    "\n",
    "    def transition_probabilities(self, state, action):\n",
    "        return self.P[state, action].items()\n",
    "\n",
    "    def rewards(self, state, action):\n",
    "        return self.R[state, action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dungeon MDP class (to complete)\n",
    "\n",
    "def pos_for_action(x, y, action):\n",
    "    if action == \"right\":\n",
    "        return x+1, y\n",
    "    if action == \"left\":\n",
    "        return x-1, y\n",
    "    if action == \"up\":\n",
    "        return x, y+1\n",
    "    if action == \"down\":\n",
    "        return x, y-1\n",
    "\n",
    "class DungeonMDP(MDP):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "                \n",
    "\n",
    "        self.walls = [\n",
    "            (0,2), (2,2), (2,3), (2, 0) \n",
    "        ]\n",
    "        for i in range(5):\n",
    "            self.walls.append((-1, i))\n",
    "            self.walls.append((5, i))\n",
    "            self.walls.append((i, -1))\n",
    "            self.walls.append((i, 5))\n",
    "        \n",
    "\n",
    "        self.holes = [\n",
    "            (0, 4), (3, 2)\n",
    "        ]\n",
    "        \n",
    "        self.stairs = (3, 4)\n",
    "                                \n",
    "        # terminal states\n",
    "        for action in self.actions():\n",
    "            for state, reward in [(\"TERMINAL\", 0), (\"WEIRD_THING\", -100), (\"HOLE\", -100), (\"STAIRS\", 0)]:\n",
    "                # all terminal states are \"absorbing\" states (you can't escape from them)\n",
    "                self.P[state, action] = {\"TERMINAL\": 1}\n",
    "                self.R[state, action] = reward\n",
    "        \n",
    "        for x in range(5):\n",
    "            for y in range(5):\n",
    "                for x_w in range(5):\n",
    "                    for y_w in range(5):\n",
    "                        for action in self.actions():\n",
    "                            # (x, y) is our position\n",
    "                            # (x_w, y_w) is the weird thing's position.\n",
    "                            \n",
    "                            curr_state = (x, y, x_w, y_w)\n",
    "                            \n",
    "                            # -1 reward for taking a step\n",
    "                            # regardless of the action\n",
    "                            self.R[curr_state, action] = -1\n",
    "                            \n",
    "                            adj = pos_for_action(x, y, action)\n",
    "                            \n",
    "                            # TODO: change how the weird thing moves by implementing weird_thing_policy().\n",
    "                            next_x_w, next_y_w = self.weird_thing_policy(x, y, x_w, y_w)\n",
    "                            # this new position is deterministically chosen by the weird thing\n",
    "                            # in opposition to the previous time, where it was randomly picked\n",
    "\n",
    "                            if adj in self.walls:\n",
    "                                next_x, next_y = x, y\n",
    "                            else:\n",
    "                                next_x, next_y = adj\n",
    "\n",
    "                            next_state = (next_x, next_y, next_x_w, next_y_w)\n",
    "                            \n",
    "                            if (next_x, next_y) == (next_x_w, next_y_w):\n",
    "                                # we encountered the weird thing\n",
    "                                self.P[curr_state, action][\"WEIRD_THING\"] = 1.\n",
    "                            elif (next_x, next_y) in self.holes:\n",
    "                                # we are moving into a hole tile\n",
    "                                # 50% to fall and 50% to continue. \n",
    "                                self.P[curr_state, action][\"HOLE\"] = 0.5\n",
    "                                self.P[curr_state, action][next_state] = 0.5\n",
    "\n",
    "                            elif (next_x, next_y) == self.stairs:\n",
    "                                # we are moving into stairs\n",
    "\n",
    "                                # if the weird thing is also at the stairs, then we lose\n",
    "                                if (next_x_w, next_y_w) == self.stairs:\n",
    "                                    self.P[curr_state, action][\"WEIRD_THING\"] = 1.\n",
    "                                else:\n",
    "                                    self.P[curr_state, action][\"STAIRS\"] = 1.\n",
    "\n",
    "                            else:\n",
    "                                # we have 100% chance to move to the next location otherwise\n",
    "                                # the probability of the next state is just given by the probability of\n",
    "                                # the weird thing moving\n",
    "                                self.P[curr_state, action][next_state] = 1.\n",
    "                                \n",
    "    def actions(self):\n",
    "        return [\"right\", \"left\", \"up\", \"down\"]\n",
    "\n",
    "    def weird_thing_policy(self, x, y, x_w, y_w):\n",
    "        # TODO: make the weird thing move toward us.\n",
    "        # If it is adjacent to us, it should not move !\n",
    "        # Use simple logic.\n",
    "        ...\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwards_inference(mdp: MDP, horizon: int) -> Tuple[List[dict], List[dict]]:\n",
    "    # This function should return the optimal value function and Q function.\n",
    "    \n",
    "    Qs = []\n",
    "    Vs = []\n",
    "    \n",
    "    # Initialize V_T(s) = 0 for all s\n",
    "    V_tp1 = {s: 0.0 for s in mdp.states()} # V_{t+1}\n",
    "    Vs.append(V_tp1)\n",
    "\n",
    "    for t in reversed(range(horizon)):\n",
    "        Qt = {}\n",
    "        Vt = {}\n",
    "        for state in mdp.states():\n",
    "            max_q = float('-inf')\n",
    "            for action in mdp.actions():\n",
    "                r = mdp.rewards(state, action)\n",
    "                expected_value = 0.0\n",
    "                for next_state, prob in mdp.transition_probabilities(state, action):\n",
    "                    expected_value += prob * V_tp1[next_state]\n",
    "                q_val = r + expected_value\n",
    "                Qt[state, action] = q_val\n",
    "                max_q = max(max_q, q_val)\n",
    "            Vt[state] = max_q\n",
    "        Qs.insert(0, Qt)\n",
    "        Vs.insert(0, Vt)\n",
    "        V_tp1 = Vt\n",
    "    \n",
    "    return Qs, Vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_policy(mdp: MDP, Qs: List[dict]) -> List[dict]:\n",
    "    # This function returns the policy didacted by the given Q function.\n",
    "    \n",
    "    policy = []\n",
    "    \n",
    "    for Qt in Qs:\n",
    "        pi_t = {}\n",
    "        for state in mdp.states():\n",
    "            best_action = max(mdp.actions(), key=lambda a: Qt[state, a])\n",
    "            pi_t[state] = best_action\n",
    "        policy.append(pi_t)\n",
    "            \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_to_vector = {\n",
    "    \"up\": (0, 0.3),\n",
    "    \"down\": (0, -0.3),\n",
    "    \"left\": (-0.3, 0),\n",
    "    \"right\": (0.3, 0),\n",
    "}\n",
    "\n",
    "def v_pi_heatmap(Vs, policy, timestep: int, weird_thing_pos: tuple, dungeon: DungeonMDP):\n",
    "    \"\"\"\n",
    "    Plots a heatmap for the value function V at a given timestep.\n",
    "    \n",
    "    \"\"\"\n",
    "    heatmap = np.full((5, 5), np.nan)\n",
    "    \n",
    "    Vt = Vs[timestep]\n",
    "    pi_t = policy[timestep]\n",
    "    \n",
    "    x_w, y_w = weird_thing_pos\n",
    "\n",
    "    # Fill in the heatmap based on fixed weird_thing_pos\n",
    "    for x in range(5):\n",
    "        for y in range(5):\n",
    "            key = (x, y, x_w, y_w)\n",
    "            if key in Vt:\n",
    "                heatmap[y, x] = Vt[key]  # Note: y first due to matplotlib row/col format\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    cmap = plt.cm.viridis\n",
    "    im = ax.imshow(heatmap, cmap=cmap, origin='lower')\n",
    "\n",
    "    # Gridlines\n",
    "    ax.set_xticks(np.arange(5))\n",
    "    ax.set_yticks(np.arange(5))\n",
    "    ax.set_xticklabels(range(5))\n",
    "    ax.set_yticklabels(range(5))\n",
    "    \n",
    "    for (x, y) in dungeon.walls:\n",
    "        ax.add_patch(plt.Rectangle((x - 0.5, y - 0.5), 1, 1, color='black'))\n",
    "\n",
    "    for (x, y) in dungeon.holes:\n",
    "        ax.add_patch(plt.Rectangle((x - 0.5, y - 0.5), 1, 1, facecolor='none', edgecolor='red', hatch='O', linewidth=1))\n",
    "\n",
    "    x, y = dungeon.stairs\n",
    "    ax.add_patch(plt.Rectangle((x - 0.5, y - 0.5), 1, 1, facecolor='none', edgecolor='green', hatch='*', linewidth=1.5))\n",
    "\n",
    "    # Plot agent and weird thing\n",
    "    ax.plot(x_w, y_w, marker='x', markersize=10, color='red', label=\"Weird Thing\")\n",
    "    \n",
    "    for x in range(5):\n",
    "        for y in range(5):\n",
    "\n",
    "            key = x, y, x_w, y_w\n",
    "            if key in pi_t:\n",
    "                action = pi_t[key]\n",
    "                dx, dy = action_to_vector[action]\n",
    "                ax.arrow(x, y, dx, dy, head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "\n",
    "    ax.set_title(f\"Value Function and policy at t={timestep}\")\n",
    "    ax.legend(loc='upper right')\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = DungeonMDP()\n",
    "\n",
    "\n",
    "T = 25\n",
    "Qs, Vs = backwards_inference(mdp, T)\n",
    "policy = compute_policy(mdp, Qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simulate(Vs, policy, mdp):\n",
    "        \n",
    "    idx = 0\n",
    "    weird_thing_pos = (4, 0) # you can change this to see how the agent reacts to \n",
    "    for t in range(1, T+1):\n",
    "        \n",
    "        v_pi_heatmap(Vs, policy, t, weird_thing_pos, mdp)\n",
    "        \n",
    "simulate(Vs, policy, mdp)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
